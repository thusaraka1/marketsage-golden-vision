# üìà ADVANCED Colombo Stock Exchange - Multi-Model Stock Prediction
# Version 3: Stacking Ensemble, CNN-LSTM, Attention Mechanism, Deep Analysis
# GPU-Accelerated Training on Google Colab (T4)

# ============================================================
# üîß STEP 1: SETUP & GPU CHECK
# ============================================================

import tensorflow as tf
print("=" * 60)
print("üîß GPU CHECK - ADVANCED MODEL v3")
print("=" * 60)
print(f"TensorFlow: {tf.__version__}")
print(f"GPU: {tf.config.list_physical_devices('GPU')}")

# ============================================================
# üìö STEP 2: IMPORT LIBRARIES
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, 
                               AdaBoostRegressor, ExtraTreesRegressor, 
                               StackingRegressor, VotingRegressor)
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import lightgbm as lgb

# Deep Learning
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (LSTM, GRU, Dense, Dropout, Input, 
                                      Bidirectional, BatchNormalization,
                                      Conv1D, MaxPooling1D, Flatten,
                                      Attention, MultiHeadAttention,
                                      GlobalAveragePooling1D, Concatenate,
                                      LayerNormalization, Add)
from tensorflow.keras.optimizers import Adam, AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l1_l2

print("‚úÖ All libraries imported!")

# ============================================================
# üìÇ STEP 3: LOAD DATA
# ============================================================

from google.colab import drive
drive.mount('/content/drive')

import zipfile
DATA_PATH = '/content/drive/MyDrive/Historical Data.zip'
with zipfile.ZipFile(DATA_PATH, 'r') as zip_ref:
    zip_ref.extractall('/content/data')
print("‚úÖ Data extracted!")

# ============================================================
# üîç STEP 4: DEEP DATA ANALYSIS
# ============================================================

def deep_data_analysis(df, company_name):
    """Comprehensive data analysis"""
    print("\n" + "=" * 60)
    print(f"üîç DEEP DATA ANALYSIS: {company_name}")
    print("=" * 60)
    
    # Basic statistics
    print(f"\nüìä Dataset Shape: {df.shape}")
    print(f"üìÖ Date Range: {df['Date'].min()} to {df['Date'].max()}")
    print(f"üí∞ Price Range: Rs.{df['Close'].min():.2f} to Rs.{df['Close'].max():.2f}")
    
    # Calculate returns
    returns = df['Close'].pct_change().dropna()
    
    print(f"\nüìà Return Statistics:")
    print(f"   Mean Daily Return: {returns.mean()*100:.4f}%")
    print(f"   Std Dev: {returns.std()*100:.4f}%")
    print(f"   Skewness: {returns.skew():.4f}")
    print(f"   Kurtosis: {returns.kurtosis():.4f}")
    print(f"   Max Daily Gain: {returns.max()*100:.2f}%")
    print(f"   Max Daily Loss: {returns.min()*100:.2f}%")
    
    # Days analysis
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Month'] = df['Date'].dt.month
    df['Quarter'] = df['Date'].dt.quarter
    
    print(f"\nüìÖ Average Returns by Day:")
    daily_returns = df.groupby('DayOfWeek')['Log_Return'].mean() * 100
    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']
    for i, ret in enumerate(daily_returns):
        if i < 5:
            print(f"   {days[i]}: {ret:.4f}%")
    
    print(f"\nüìÖ Average Returns by Month:")
    monthly_returns = df.groupby('Month')['Log_Return'].mean() * 100
    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    for month, ret in monthly_returns.items():
        print(f"   {months[month-1]}: {ret:.4f}%")
    
    return df

# ============================================================
# üîß STEP 5: ADVANCED FEATURE ENGINEERING
# ============================================================

def adjust_stock_split(df, split_date='2024-11-01', split_ratio=10):
    """Adjust for stock split"""
    df = df.copy()
    split_date = pd.to_datetime(split_date)
    price_columns = ['Open', 'High', 'Low', 'Close']
    mask = df['Date'] < split_date
    for col in price_columns:
        df.loc[mask, col] = df.loc[mask, col] / split_ratio
    print(f"‚úÖ Adjusted {mask.sum()} rows for {split_ratio}:1 stock split")
    return df

def create_advanced_features(df):
    """Create comprehensive feature set"""
    df = df.copy()
    
    # === PRICE-BASED FEATURES ===
    df['Price_Range'] = (df['High'] - df['Low']) / df['Close']
    df['Price_Change'] = df['Close'].pct_change()
    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    df['High_Low_Ratio'] = df['High'] / df['Low']
    df['Open_Close_Ratio'] = df['Open'] / df['Close']
    
    # === MULTIPLE MOVING AVERAGES ===
    for window in [5, 10, 20, 50]:
        df[f'MA_{window}'] = df['Close'].rolling(window=window).mean()
        df[f'MA_{window}_Ratio'] = df['Close'] / df[f'MA_{window}']
        df[f'MA_{window}_Slope'] = df[f'MA_{window}'].diff(5) / df[f'MA_{window}'].shift(5)
    
    # === EMA (Exponential Moving Average) ===
    for span in [12, 26, 50]:
        df[f'EMA_{span}'] = df['Close'].ewm(span=span, adjust=False).mean()
        df[f'EMA_{span}_Ratio'] = df['Close'] / df[f'EMA_{span}']
    
    # === RSI (Multiple periods) ===
    for period in [7, 14, 21]:
        delta = df['Close'].diff()
        gain = delta.where(delta > 0, 0).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / (loss + 1e-10)
        df[f'RSI_{period}'] = 100 - (100 / (1 + rs))
    
    # === MACD ===
    exp12 = df['Close'].ewm(span=12, adjust=False).mean()
    exp26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp12 - exp26
    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']
    df['MACD_Hist_Normalized'] = df['MACD_Hist'] / df['Close']
    
    # === Bollinger Bands ===
    for window in [10, 20]:
        bb_middle = df['Close'].rolling(window=window).mean()
        bb_std = df['Close'].rolling(window=window).std()
        df[f'BB_{window}_Upper'] = bb_middle + 2 * bb_std
        df[f'BB_{window}_Lower'] = bb_middle - 2 * bb_std
        df[f'BB_{window}_Width'] = (df[f'BB_{window}_Upper'] - df[f'BB_{window}_Lower']) / bb_middle
        df[f'BB_{window}_Position'] = (df['Close'] - df[f'BB_{window}_Lower']) / (df[f'BB_{window}_Upper'] - df[f'BB_{window}_Lower'] + 1e-10)
    
    # === Stochastic Oscillator ===
    for period in [14, 21]:
        low_min = df['Low'].rolling(window=period).min()
        high_max = df['High'].rolling(window=period).max()
        df[f'Stoch_K_{period}'] = 100 * (df['Close'] - low_min) / (high_max - low_min + 1e-10)
        df[f'Stoch_D_{period}'] = df[f'Stoch_K_{period}'].rolling(window=3).mean()
    
    # === ATR (Average True Range) ===
    for period in [7, 14]:
        high_low = df['High'] - df['Low']
        high_close = abs(df['High'] - df['Close'].shift(1))
        low_close = abs(df['Low'] - df['Close'].shift(1))
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df[f'ATR_{period}'] = true_range.rolling(window=period).mean()
        df[f'ATR_{period}_Normalized'] = df[f'ATR_{period}'] / df['Close']
    
    # === Volatility (Multiple periods) ===
    for window in [5, 10, 20]:
        df[f'Volatility_{window}'] = df['Log_Return'].rolling(window=window).std()
        df[f'Volatility_{window}_Rank'] = df[f'Volatility_{window}'].rolling(window=50).apply(
            lambda x: pd.Series(x).rank().iloc[-1] / len(x), raw=False
        )
    
    # === Volume Features ===
    df['Volume_MA_5'] = df['ShareVolume'].rolling(window=5).mean()
    df['Volume_MA_20'] = df['ShareVolume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['ShareVolume'] / (df['Volume_MA_5'] + 1)
    df['Volume_Trend'] = df['Volume_MA_5'] / (df['Volume_MA_20'] + 1)
    
    # === Momentum Features ===
    for period in [5, 10, 20]:
        df[f'Momentum_{period}'] = df['Close'] / df['Close'].shift(period) - 1
        df[f'ROC_{period}'] = (df['Close'] - df['Close'].shift(period)) / df['Close'].shift(period) * 100
    
    # === Williams %R ===
    for period in [14, 21]:
        high_max = df['High'].rolling(window=period).max()
        low_min = df['Low'].rolling(window=period).min()
        df[f'Williams_R_{period}'] = -100 * (high_max - df['Close']) / (high_max - low_min + 1e-10)
    
    # === CCI (Commodity Channel Index) ===
    for period in [14, 20]:
        typical_price = (df['High'] + df['Low'] + df['Close']) / 3
        sma = typical_price.rolling(window=period).mean()
        mad = typical_price.rolling(window=period).apply(lambda x: np.abs(x - x.mean()).mean(), raw=True)
        df[f'CCI_{period}'] = (typical_price - sma) / (0.015 * mad + 1e-10)
    
    # === OBV (On-Balance Volume) - Trend ===
    obv = np.where(df['Close'] > df['Close'].shift(1), df['ShareVolume'], 
                   np.where(df['Close'] < df['Close'].shift(1), -df['ShareVolume'], 0))
    df['OBV'] = np.cumsum(obv)
    df['OBV_MA'] = df['OBV'].rolling(window=20).mean()
    df['OBV_Trend'] = df['OBV'] / (df['OBV_MA'] + 1e-10)
    
    # === Calendar Features ===
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Month'] = df['Date'].dt.month
    df['Quarter'] = df['Date'].dt.quarter
    df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)
    df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)
    
    # === Lag Features ===
    for lag in [1, 2, 3, 5]:
        df[f'Return_Lag_{lag}'] = df['Log_Return'].shift(lag)
        df[f'Close_Lag_{lag}_Ratio'] = df['Close'] / df['Close'].shift(lag)
    
    return df

def load_and_preprocess_advanced(company_folder):
    """Load and preprocess with advanced features"""
    df = pd.read_csv(f'{company_folder}/Daily.csv')
    df.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'TradeVolume', 'ShareVolume', 'Turnover']
    
    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y')
    df = df.sort_values('Date').reset_index(drop=True)
    df = df.drop_duplicates(subset='Date', keep='first')
    
    # Fill missing
    df['Open'] = df['Open'].fillna(df['Close'])
    df['High'] = df['High'].fillna(df['Close'])
    df['Low'] = df['Low'].fillna(df['Close'])
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    # Adjust for stock split
    df = adjust_stock_split(df)
    
    # Create features
    df = create_advanced_features(df)
    
    # Target
    df['Target_Return'] = df['Close'].shift(-1) / df['Close'] - 1
    
    # Drop NaN
    df = df.dropna()
    
    return df

# ============================================================
# üìä STEP 6: LOAD DATA
# ============================================================

companies = {
    'JKH': '/content/data/Historical Data/Historical data 2020-2025/JKH.N0000 - John Keells',
    'COMB': '/content/data/Historical Data/Historical data 2020-2025/COMB.N0000 - Commercial Bank',
    'CTC': '/content/data/Historical Data/Historical data 2020-2025/CTC.N0000 - Ceylon Tobacco',
    'DIAL': '/content/data/Historical Data/Historical data 2020-2025/DIAL.N0000 - Dialog Axiata',
    'DIST': '/content/data/Historical Data/Historical data 2020-2025/DIST.N0000 - Distilleries Company'
}

COMPANY = 'JKH'
df = load_and_preprocess_advanced(companies[COMPANY])
df = deep_data_analysis(df, COMPANY)

print(f"\nüìä Total Features Created: {df.shape[1] - 3}")  # Exclude Date, Target, Close

# ============================================================
# üéØ STEP 7: FEATURE SELECTION
# ============================================================

# Select best features (exclude date, target, and raw prices)
exclude_cols = ['Date', 'Target_Return', 'Open', 'High', 'Low', 'Close', 
                'TradeVolume', 'ShareVolume', 'Turnover', 'OBV',
                'MA_5', 'MA_10', 'MA_20', 'MA_50', 'EMA_12', 'EMA_26', 'EMA_50',
                'BB_10_Upper', 'BB_10_Lower', 'BB_20_Upper', 'BB_20_Lower',
                'Volume_MA_5', 'Volume_MA_20', 'OBV_MA']

feature_columns = [col for col in df.columns if col not in exclude_cols]
print(f"\nüéØ Selected Features: {len(feature_columns)}")

X = df[feature_columns].values
y = df['Target_Return'].values

print(f"Feature matrix shape: {X.shape}")

# ============================================================
# üìä STEP 8: FEATURE IMPORTANCE ANALYSIS
# ============================================================

print("\n" + "=" * 60)
print("üìä FEATURE IMPORTANCE ANALYSIS")
print("=" * 60)

# Quick RF for feature importance
from sklearn.ensemble import RandomForestRegressor
rf_temp = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf_temp.fit(X, y)

feature_importance = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': rf_temp.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nüîë Top 20 Most Important Features:")
for i, row in feature_importance.head(20).iterrows():
    print(f"   {row['Feature']}: {row['Importance']:.4f}")

# Use top features
TOP_N_FEATURES = 30
top_features = feature_importance.head(TOP_N_FEATURES)['Feature'].tolist()
X_selected = df[top_features].values

print(f"\n‚úÖ Using top {TOP_N_FEATURES} features for models")

# ============================================================
# üîÄ STEP 9: TRAIN/TEST SPLIT
# ============================================================

scaler_X = RobustScaler()
scaler_y = RobustScaler()

X_scaled = scaler_X.fit_transform(X_selected)
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

split_idx = int(len(X_scaled) * 0.8)
X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]
y_train_orig = y[:split_idx]
y_test_orig = y[split_idx:]

print(f"\nüìä Training: {len(X_train)} | Testing: {len(X_test)}")

# ============================================================
# ü§ñ STEP 10: ML MODELS WITH HYPERPARAMETER TUNING
# ============================================================

print("\n" + "=" * 60)
print("ü§ñ TRAINING ML MODELS WITH TUNING")
print("=" * 60)

# 1. XGBoost
print("\n1Ô∏è‚É£ Training XGBoost...")
xgb_model = xgb.XGBRegressor(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)
xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
xgb_pred = xgb_model.predict(X_test)

# 2. LightGBM
print("2Ô∏è‚É£ Training LightGBM...")
lgb_model = lgb.LGBMRegressor(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42,
    verbosity=-1
)
lgb_model.fit(X_train, y_train)
lgb_pred = lgb_model.predict(X_test)

# 3. Extra Trees
print("3Ô∏è‚É£ Training Extra Trees...")
et_model = ExtraTreesRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)
et_model.fit(X_train, y_train)
et_pred = et_model.predict(X_test)

# 4. Gradient Boosting
print("4Ô∏è‚É£ Training Gradient Boosting...")
gb_model = GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)

# 5. SVR
print("5Ô∏è‚É£ Training SVR...")
svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.01)
svr_model.fit(X_train, y_train)
svr_pred = svr_model.predict(X_test)

# ============================================================
# üß† STEP 11: DEEP LEARNING MODELS
# ============================================================

SEQ_LENGTH = 30

def create_sequences(X, y, seq_length=30):
    Xs, ys = [], []
    for i in range(len(X) - seq_length):
        Xs.append(X[i:i+seq_length])
        ys.append(y[i+seq_length])
    return np.array(Xs), np.array(ys)

X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)
split_seq = int(len(X_seq) * 0.8)
X_train_seq, X_test_seq = X_seq[:split_seq], X_seq[split_seq:]
y_train_seq, y_test_seq = y_seq[:split_seq], y_seq[split_seq:]

print(f"\nüß† Sequence shapes: Train {X_train_seq.shape}, Test {X_test_seq.shape}")

callbacks = [
    EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
]

# Model 1: Deep LSTM
print("\n" + "=" * 60)
print("üß† MODEL: DEEP LSTM (3 layers)")
print("=" * 60)

def build_deep_lstm(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(0.001, 0.001)),
        BatchNormalization(),
        Dropout(0.3),
        LSTM(64, return_sequences=True, kernel_regularizer=l1_l2(0.001, 0.001)),
        BatchNormalization(),
        Dropout(0.3),
        LSTM(32, return_sequences=False),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer=AdamW(learning_rate=0.001), loss='huber', metrics=['mae'])
    return model

deep_lstm = build_deep_lstm((SEQ_LENGTH, X_train_seq.shape[2]))
history_lstm = deep_lstm.fit(X_train_seq, y_train_seq, epochs=150, batch_size=32,
                              validation_split=0.2, callbacks=callbacks, verbose=1)
deep_lstm_pred = deep_lstm.predict(X_test_seq).flatten()

# Model 2: CNN-LSTM Hybrid
print("\n" + "=" * 60)
print("üîó MODEL: CNN-LSTM HYBRID")
print("=" * 60)

def build_cnn_lstm(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Conv1D(64, kernel_size=3, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        Conv1D(32, kernel_size=3, activation='relu', padding='same'),
        BatchNormalization(),
        LSTM(64, return_sequences=False),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer=AdamW(learning_rate=0.001), loss='huber', metrics=['mae'])
    return model

cnn_lstm = build_cnn_lstm((SEQ_LENGTH, X_train_seq.shape[2]))
history_cnn = cnn_lstm.fit(X_train_seq, y_train_seq, epochs=150, batch_size=32,
                           validation_split=0.2, callbacks=callbacks, verbose=1)
cnn_lstm_pred = cnn_lstm.predict(X_test_seq).flatten()

# Model 3: Bidirectional GRU
print("\n" + "=" * 60)
print("üîÑ MODEL: BIDIRECTIONAL GRU")
print("=" * 60)

def build_bigru(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Bidirectional(GRU(64, return_sequences=True)),
        BatchNormalization(),
        Dropout(0.3),
        Bidirectional(GRU(32, return_sequences=False)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer=AdamW(learning_rate=0.001), loss='huber', metrics=['mae'])
    return model

bigru = build_bigru((SEQ_LENGTH, X_train_seq.shape[2]))
history_gru = bigru.fit(X_train_seq, y_train_seq, epochs=150, batch_size=32,
                        validation_split=0.2, callbacks=callbacks, verbose=1)
bigru_pred = bigru.predict(X_test_seq).flatten()

# Model 4: Transformer-like with Multi-Head Attention
print("\n" + "=" * 60)
print("üéØ MODEL: LSTM WITH ATTENTION")
print("=" * 60)

def build_attention_lstm(input_shape):
    inputs = Input(shape=input_shape)
    
    # LSTM layer
    lstm_out = LSTM(64, return_sequences=True)(inputs)
    lstm_out = BatchNormalization()(lstm_out)
    
    # Self-attention
    attention_output = MultiHeadAttention(num_heads=4, key_dim=16)(lstm_out, lstm_out)
    attention_output = Dropout(0.3)(attention_output)
    attention_output = Add()([lstm_out, attention_output])
    attention_output = LayerNormalization()(attention_output)
    
    # Final layers
    x = GlobalAveragePooling1D()(attention_output)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(1)(x)
    
    model = Model(inputs, outputs)
    model.compile(optimizer=AdamW(learning_rate=0.001), loss='huber', metrics=['mae'])
    return model

attention_lstm = build_attention_lstm((SEQ_LENGTH, X_train_seq.shape[2]))
history_att = attention_lstm.fit(X_train_seq, y_train_seq, epochs=150, batch_size=32,
                                  validation_split=0.2, callbacks=callbacks, verbose=1)
attention_pred = attention_lstm.predict(X_test_seq).flatten()

# ============================================================
# üéØ STEP 12: STACKING ENSEMBLE
# ============================================================

print("\n" + "=" * 60)
print("üèÜ STACKING ENSEMBLE")
print("=" * 60)

# Inverse transform DL predictions
deep_lstm_pred_inv = scaler_y.inverse_transform(deep_lstm_pred.reshape(-1, 1)).flatten()
cnn_lstm_pred_inv = scaler_y.inverse_transform(cnn_lstm_pred.reshape(-1, 1)).flatten()
bigru_pred_inv = scaler_y.inverse_transform(bigru_pred.reshape(-1, 1)).flatten()
attention_pred_inv = scaler_y.inverse_transform(attention_pred.reshape(-1, 1)).flatten()

# ML predictions (aligned to DL test set)
test_offset = len(y_test) - len(y_test_seq)
xgb_pred_aligned = scaler_y.inverse_transform(xgb_pred[test_offset:].reshape(-1, 1)).flatten()
lgb_pred_aligned = scaler_y.inverse_transform(lgb_pred[test_offset:].reshape(-1, 1)).flatten()
et_pred_aligned = scaler_y.inverse_transform(et_pred[test_offset:].reshape(-1, 1)).flatten()
gb_pred_aligned = scaler_y.inverse_transform(gb_pred[test_offset:].reshape(-1, 1)).flatten()

# Stack all predictions
stacked_features = np.column_stack([
    deep_lstm_pred_inv, cnn_lstm_pred_inv, bigru_pred_inv, attention_pred_inv,
    xgb_pred_aligned, lgb_pred_aligned, et_pred_aligned, gb_pred_aligned
])

# Meta-learner: Ridge regression on stacked predictions
from sklearn.model_selection import cross_val_predict
meta_model = Ridge(alpha=1.0)

# Use portion of test set for training meta-model (careful with data leakage)
y_test_dl = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()

# Simple 80/20 split on test set for meta-model
meta_split = int(len(stacked_features) * 0.5)
meta_model.fit(stacked_features[:meta_split], y_test_dl[:meta_split])
stacked_pred = meta_model.predict(stacked_features[meta_split:])

# Simple average ensemble
simple_ensemble = (deep_lstm_pred_inv + cnn_lstm_pred_inv + bigru_pred_inv + 
                   attention_pred_inv + xgb_pred_aligned + lgb_pred_aligned) / 6

# Weighted ensemble (give more weight to better models)
weighted_ensemble = (0.25 * deep_lstm_pred_inv + 0.20 * cnn_lstm_pred_inv + 
                     0.20 * bigru_pred_inv + 0.15 * attention_pred_inv +
                     0.10 * xgb_pred_aligned + 0.10 * lgb_pred_aligned)

# ============================================================
# üìä STEP 13: EVALUATION
# ============================================================

def evaluate_model(y_true, y_pred, model_name):
    """Comprehensive evaluation"""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred)) * 100
    mae = mean_absolute_error(y_true, y_pred) * 100
    r2 = r2_score(y_true, y_pred)
    
    dir_true = y_true > 0
    dir_pred = y_pred > 0
    dir_acc = np.mean(dir_true == dir_pred) * 100
    
    return {
        'Model': model_name,
        'RMSE (%)': rmse,
        'MAE (%)': mae,
        'R¬≤ Score': r2,
        'Directional Acc (%)': dir_acc
    }

print("\n" + "=" * 70)
print("üìä FINAL MODEL COMPARISON")
print("=" * 70)

results = []

# ML models
y_test_ml = y_test_orig[test_offset:]
results.append(evaluate_model(y_test_ml, xgb_pred_aligned[:len(y_test_ml)], 'XGBoost'))
results.append(evaluate_model(y_test_ml, lgb_pred_aligned[:len(y_test_ml)], 'LightGBM'))
results.append(evaluate_model(y_test_ml, et_pred_aligned[:len(y_test_ml)], 'Extra Trees'))
results.append(evaluate_model(y_test_ml, gb_pred_aligned[:len(y_test_ml)], 'Gradient Boosting'))

# DL models
results.append(evaluate_model(y_test_dl, deep_lstm_pred_inv, 'Deep LSTM (3L)'))
results.append(evaluate_model(y_test_dl, cnn_lstm_pred_inv, 'CNN-LSTM'))
results.append(evaluate_model(y_test_dl, bigru_pred_inv, 'BiGRU'))
results.append(evaluate_model(y_test_dl, attention_pred_inv, 'LSTM + Attention'))

# Ensembles
results.append(evaluate_model(y_test_dl, simple_ensemble, 'Simple Ensemble (Avg)'))
results.append(evaluate_model(y_test_dl, weighted_ensemble, 'Weighted Ensemble'))

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Directional Acc (%)', ascending=False)

print(results_df.to_string(index=False))

# ============================================================
# üìà STEP 14: VISUALIZATION
# ============================================================

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Directional Accuracy
ax1 = axes[0, 0]
colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(results_df)))
bars = ax1.bar(range(len(results_df)), results_df['Directional Acc (%)'], color=colors)
ax1.axhline(y=50, color='red', linestyle='--', linewidth=2, label='Random (50%)')
ax1.set_xticks(range(len(results_df)))
ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax1.set_title('Directional Accuracy by Model', fontsize=14, fontweight='bold')
ax1.set_ylabel('Accuracy (%)')
ax1.legend()
for bar, val in zip(bars, results_df['Directional Acc (%)']):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}%', 
             ha='center', va='bottom', fontsize=8)

# 2. R¬≤ Score
ax2 = axes[0, 1]
bars = ax2.bar(range(len(results_df)), results_df['R¬≤ Score'], color=colors)
ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)
ax2.set_xticks(range(len(results_df)))
ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax2.set_title('R¬≤ Score by Model', fontsize=14, fontweight='bold')
ax2.set_ylabel('R¬≤ Score')

# 3. Training curves
ax3 = axes[1, 0]
ax3.plot(history_lstm.history['loss'], label='Deep LSTM')
ax3.plot(history_cnn.history['loss'], label='CNN-LSTM')
ax3.plot(history_gru.history['loss'], label='BiGRU')
ax3.plot(history_att.history['loss'], label='Attention')
ax3.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')
ax3.set_xlabel('Epoch')
ax3.set_ylabel('Loss')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. Actual vs Best Prediction
best_model_name = results_df.iloc[0]['Model']
ax4 = axes[1, 1]
ax4.plot(y_test_dl, 'b-', alpha=0.7, label='Actual', linewidth=1)
ax4.plot(weighted_ensemble, 'r-', alpha=0.7, label='Weighted Ensemble', linewidth=1)
ax4.set_title('Actual vs Predicted Returns', fontsize=14, fontweight='bold')
ax4.set_xlabel('Time')
ax4.set_ylabel('Return')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/content/advanced_model_comparison.png', dpi=300)
plt.show()

# ============================================================
# üíæ STEP 15: SAVE MODELS
# ============================================================

import pickle
import os

save_path = '/content/drive/MyDrive/CSE_Models_v3'
os.makedirs(save_path, exist_ok=True)

deep_lstm.save(f'{save_path}/{COMPANY}_deep_lstm.keras')
cnn_lstm.save(f'{save_path}/{COMPANY}_cnn_lstm.keras')
bigru.save(f'{save_path}/{COMPANY}_bigru.keras')
attention_lstm.save(f'{save_path}/{COMPANY}_attention.keras')

with open(f'{save_path}/{COMPANY}_xgb.pkl', 'wb') as f: pickle.dump(xgb_model, f)
with open(f'{save_path}/{COMPANY}_lgb.pkl', 'wb') as f: pickle.dump(lgb_model, f)
with open(f'{save_path}/{COMPANY}_et.pkl', 'wb') as f: pickle.dump(et_model, f)
with open(f'{save_path}/{COMPANY}_scalers_v3.pkl', 'wb') as f:
    pickle.dump({'scaler_X': scaler_X, 'scaler_y': scaler_y, 'top_features': top_features}, f)

results_df.to_csv(f'{save_path}/{COMPANY}_results_v3.csv', index=False)

print(f"\n‚úÖ All models saved to {save_path}")

# ============================================================
# üìã FINAL SUMMARY
# ============================================================

print("\n" + "=" * 70)
print("üìã FINAL SUMMARY - ADVANCED MODEL v3")
print("=" * 70)

best = results_df.iloc[0]
print(f"\nüèÜ BEST MODEL: {best['Model']}")
print(f"   ‚Ä¢ Directional Accuracy: {best['Directional Acc (%)']:.1f}%")
print(f"   ‚Ä¢ R¬≤ Score: {best['R¬≤ Score']:.4f}")
print(f"   ‚Ä¢ RMSE: {best['RMSE (%)']:.4f}%")

print(f"\nüìä Features used: {len(top_features)}")
print(f"üìä Total models trained: 10")
print(f"üìä Best 3 models:")
for i, row in results_df.head(3).iterrows():
    print(f"   {i+1}. {row['Model']}: {row['Directional Acc (%)']:.1f}%")

if best['Directional Acc (%)'] > 55:
    print(f"\n‚úÖ Excellent! Model beats random guess significantly!")
if best['Directional Acc (%)'] > 60:
    print(f"‚úÖ Very good performance for academic research!")
if best['Directional Acc (%)'] > 65:
    print(f"üèÜ Outstanding! This is publishable-quality accuracy!")

print("=" * 70)
