# üìà OPTIMIZED Colombo Stock Exchange - Push Beyond 63%
# Combines: More Data + Best Features + Hyperparameter Tuning + Hybrid Ensemble
# GPU-Accelerated on Google Colab T4

# ============================================================
# üîß SETUP
# ============================================================

import tensorflow as tf
print("=" * 60)
print("üöÄ OPTIMIZED MODEL - TARGET: 65%+ ACCURACY")
print("=" * 60)
print(f"GPU: {tf.config.list_physical_devices('GPU')}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout, Input, BatchNormalization, Bidirectional
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

print("‚úÖ Libraries imported!")

# ============================================================
# üìÇ LOAD ALL DATA (2015-2025 = 10 YEARS!)
# ============================================================

from google.colab import drive
drive.mount('/content/drive')

import zipfile
DATA_PATH = '/content/drive/MyDrive/Historical Data.zip'
with zipfile.ZipFile(DATA_PATH, 'r') as zip_ref:
    zip_ref.extractall('/content/data')

def load_full_dataset(company_name='JKH'):
    """Load and combine 2015-2020 AND 2020-2025 data"""
    
    base_2015 = f'/content/data/Historical Data/Historical data 2015-2020/{company_name}.N0000'
    base_2020 = f'/content/data/Historical Data/Historical data 2020-2025/{company_name}.N0000'
    
    if company_name == 'JKH':
        base_2015 += ' - John Keells'
        base_2020 += ' - John Keells'
    elif company_name == 'COMB':
        base_2015 += ' - Commercial Bank'
        base_2020 += ' - Commercial Bank'
    elif company_name == 'CTC':
        base_2015 += ' - Ceylon Tobacco'
        base_2020 += ' - Ceylon Tobacco'
    elif company_name == 'DIAL':
        base_2015 += ' - Dialog Axiata'
        base_2020 += ' - Dialog Axiata'
    elif company_name == 'DIST':
        base_2015 += ' - Distilleries Company'
        base_2020 += ' - Distilleries Company'
    
    # Load both datasets
    df1 = pd.read_csv(f'{base_2015}/Daily.csv')
    df2 = pd.read_csv(f'{base_2020}/Daily.csv')
    
    # Combine
    df = pd.concat([df1, df2], ignore_index=True)
    df.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'TradeVolume', 'ShareVolume', 'Turnover']
    
    # Parse dates
    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y')
    df = df.sort_values('Date').reset_index(drop=True)
    df = df.drop_duplicates(subset='Date', keep='first')
    
    print(f"‚úÖ Loaded {len(df)} total records (2015-2025)")
    return df

def adjust_for_stock_split(df):
    """Adjust prices for stock split"""
    df = df.copy()
    split_date = pd.to_datetime('2024-11-01')
    mask = df['Date'] < split_date
    for col in ['Open', 'High', 'Low', 'Close']:
        df.loc[mask, col] = df.loc[mask, col] / 10
    df['Open'] = df['Open'].fillna(df['Close'])
    df['High'] = df['High'].fillna(df['Close'])
    df['Low'] = df['Low'].fillna(df['Close'])
    df = df.fillna(method='ffill').fillna(method='bfill')
    print(f"‚úÖ Adjusted {mask.sum()} rows for stock split")
    return df

def create_optimized_features(df):
    """Create the BEST features from v2 (proven) + selected v3 features"""
    df = df.copy()
    
    # === PROVEN FEATURES FROM V2 (63.2% accuracy) ===
    df['Price_Change'] = df['Close'].pct_change()
    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    
    # Moving Average Ratios
    df['MA_5'] = df['Close'].rolling(window=5).mean()
    df['MA_10'] = df['Close'].rolling(window=10).mean()
    df['MA_20'] = df['Close'].rolling(window=20).mean()
    df['Close_MA5_Ratio'] = df['Close'] / df['MA_5']
    df['Close_MA10_Ratio'] = df['Close'] / df['MA_10']
    df['Close_MA20_Ratio'] = df['Close'] / df['MA_20']
    
    # RSI
    delta = df['Close'].diff()
    gain = delta.where(delta > 0, 0).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    df['RSI'] = 100 - (100 / (1 + gain / (loss + 1e-10)))
    
    # MACD
    exp12 = df['Close'].ewm(span=12, adjust=False).mean()
    exp26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp12 - exp26
    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']
    
    # Bollinger Bands Position
    bb_middle = df['Close'].rolling(window=20).mean()
    bb_std = df['Close'].rolling(window=20).std()
    bb_upper = bb_middle + 2 * bb_std
    bb_lower = bb_middle - 2 * bb_std
    df['BB_Position'] = (df['Close'] - bb_lower) / (bb_upper - bb_lower + 1e-10)
    
    # Volatility
    df['Volatility_5'] = df['Log_Return'].rolling(window=5).std()
    df['Volatility_20'] = df['Log_Return'].rolling(window=20).std()
    
    # Volume
    df['Volume_MA_5'] = df['ShareVolume'].rolling(window=5).mean()
    df['Volume_Ratio'] = df['ShareVolume'] / (df['Volume_MA_5'] + 1)
    
    # Momentum
    df['Momentum_5'] = df['Close'] / df['Close'].shift(5) - 1
    df['Momentum_10'] = df['Close'] / df['Close'].shift(10) - 1
    
    # === ADDITIONAL SELECTED FEATURES FROM V3 ===
    # (Only the ones that showed importance > 0.02)
    
    # ATR (Top feature in v3)
    high_low = df['High'] - df['Low']
    high_close = abs(df['High'] - df['Close'].shift(1))
    low_close = abs(df['Low'] - df['Close'].shift(1))
    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df['ATR_7'] = true_range.rolling(window=7).mean()
    df['ATR_Normalized'] = df['ATR_7'] / df['Close']
    
    # Price ratios (2nd top in v3)
    df['Open_Close_Ratio'] = df['Open'] / df['Close']
    df['High_Low_Ratio'] = df['High'] / df['Low']
    
    # Lag returns
    df['Return_Lag_1'] = df['Log_Return'].shift(1)
    df['Return_Lag_2'] = df['Log_Return'].shift(2)
    df['Return_Lag_3'] = df['Log_Return'].shift(3)
    
    # Day of week effect (found in analysis)
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['IsFriday'] = (df['DayOfWeek'] == 4).astype(int)  # Friday best
    df['IsMonday'] = (df['DayOfWeek'] == 0).astype(int)  # Monday worst
    
    # Target
    df['Target_Return'] = df['Close'].shift(-1) / df['Close'] - 1
    
    return df.dropna()

# ============================================================
# üìä LOAD AND PREPARE DATA
# ============================================================

COMPANY = 'JKH'
df = load_full_dataset(COMPANY)
df = adjust_for_stock_split(df)
df = create_optimized_features(df)

print(f"\nüìä Final dataset: {len(df)} records")
print(f"üìÖ Date range: {df['Date'].min()} to {df['Date'].max()}")

# Best features (from v2 + selected v3)
feature_columns = [
    'Close_MA5_Ratio', 'Close_MA10_Ratio', 'Close_MA20_Ratio',
    'RSI', 'MACD_Hist', 'BB_Position',
    'Volatility_5', 'Volatility_20',
    'Volume_Ratio', 'Momentum_5', 'Momentum_10', 'Price_Change',
    'ATR_Normalized', 'Open_Close_Ratio', 'High_Low_Ratio',
    'Return_Lag_1', 'Return_Lag_2', 'Return_Lag_3',
    'IsFriday', 'IsMonday'
]

X = df[feature_columns].values
y = df['Target_Return'].values

print(f"üéØ Features: {X.shape[1]}")

# Scale
scaler_X = RobustScaler()
scaler_y = RobustScaler()
X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Split (chronological)
split_idx = int(len(X_scaled) * 0.85)  # More training data
X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]
y_test_orig = y[split_idx:]

print(f"üìä Training: {len(X_train)} | Testing: {len(X_test)}")

# ============================================================
# ü§ñ ML MODEL: OPTIMIZED EXTRA TREES
# ============================================================

print("\n" + "=" * 60)
print("üå≤ EXTRA TREES WITH GRID SEARCH")
print("=" * 60)

# Hyperparameter tuning
param_grid = {
    'n_estimators': [200, 300, 500],
    'max_depth': [8, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

tscv = TimeSeriesSplit(n_splits=3)
et_base = ExtraTreesRegressor(random_state=42, n_jobs=-1)

# Quick grid search (reduced for speed)
print("Tuning hyperparameters...")
best_score = -999
best_params = {}

for n_est in [200, 300]:
    for max_d in [10, 15]:
        et_temp = ExtraTreesRegressor(
            n_estimators=n_est, max_depth=max_d, 
            min_samples_split=5, random_state=42, n_jobs=-1
        )
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            et_temp.fit(X_train[train_idx], y_train[train_idx])
            pred = et_temp.predict(X_train[val_idx])
            dir_acc = np.mean((y_train[val_idx] > 0) == (pred > 0))
            scores.append(dir_acc)
        mean_score = np.mean(scores)
        if mean_score > best_score:
            best_score = mean_score
            best_params = {'n_estimators': n_est, 'max_depth': max_d}

print(f"Best params: {best_params}, CV Score: {best_score:.4f}")

et_model = ExtraTreesRegressor(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)
et_model.fit(X_train, y_train)
et_pred = et_model.predict(X_test)

# ============================================================
# üß† DL: OPTIMIZED GRU (Best from v2)
# ============================================================

print("\n" + "=" * 60)
print("üß† OPTIMIZED GRU")
print("=" * 60)

SEQ_LENGTH = 30

def create_sequences(X, y, seq_length=30):
    Xs, ys = [], []
    for i in range(len(X) - seq_length):
        Xs.append(X[i:i+seq_length])
        ys.append(y[i+seq_length])
    return np.array(Xs), np.array(ys)

X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)
split_seq = int(len(X_seq) * 0.85)
X_train_seq, X_test_seq = X_seq[:split_seq], X_seq[split_seq:]
y_train_seq, y_test_seq = y_seq[:split_seq], y_seq[split_seq:]

print(f"Sequence shape: {X_train_seq.shape}")

# Multiple GRU configurations to try
gru_configs = [
    {'units': [64, 32], 'dropout': 0.2, 'name': 'GRU-64-32'},
    {'units': [128, 64], 'dropout': 0.3, 'name': 'GRU-128-64'},
    {'units': [64, 32, 16], 'dropout': 0.25, 'name': 'GRU-64-32-16'},
]

def build_gru(input_shape, units, dropout):
    model = Sequential([Input(shape=input_shape)])
    for i, u in enumerate(units):
        return_seq = (i < len(units) - 1)
        model.add(GRU(u, return_sequences=return_seq))
        model.add(BatchNormalization())
        model.add(Dropout(dropout))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer=AdamW(learning_rate=0.001), loss='huber', metrics=['mae'])
    return model

callbacks = [
    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
]

best_gru_acc = 0
best_gru_model = None
best_gru_pred = None
best_gru_name = ""

for config in gru_configs:
    print(f"\nTraining {config['name']}...")
    model = build_gru((SEQ_LENGTH, X_train_seq.shape[2]), config['units'], config['dropout'])
    model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32,
              validation_split=0.15, callbacks=callbacks, verbose=0)
    pred = model.predict(X_test_seq, verbose=0).flatten()
    pred_inv = scaler_y.inverse_transform(pred.reshape(-1, 1)).flatten()
    y_test_dl = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()
    dir_acc = np.mean((y_test_dl > 0) == (pred_inv > 0)) * 100
    print(f"  {config['name']}: {dir_acc:.1f}%")
    
    if dir_acc > best_gru_acc:
        best_gru_acc = dir_acc
        best_gru_model = model
        best_gru_pred = pred_inv
        best_gru_name = config['name']

print(f"\nüèÜ Best GRU: {best_gru_name} with {best_gru_acc:.1f}%")

# ============================================================
# üîó HYBRID ENSEMBLE: ML + DL
# ============================================================

print("\n" + "=" * 60)
print("üîó HYBRID ENSEMBLE (ML + DL)")
print("=" * 60)

# Align predictions (ET is on full test, GRU is on sequence test)
y_test_dl = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()
et_pred_inv = scaler_y.inverse_transform(et_pred.reshape(-1, 1)).flatten()
test_offset = len(y_test) - len(y_test_seq)
et_pred_aligned = et_pred_inv[test_offset:]

# Different ensemble weights to try
weights_to_try = [
    (0.3, 0.7, "30% ET, 70% GRU"),
    (0.5, 0.5, "50% ET, 50% GRU"),
    (0.4, 0.6, "40% ET, 60% GRU"),
    (0.6, 0.4, "60% ET, 40% GRU"),
]

best_ensemble_acc = 0
best_ensemble_pred = None
best_weights = ""

for w_et, w_gru, name in weights_to_try:
    ensemble_pred = w_et * et_pred_aligned + w_gru * best_gru_pred
    dir_acc = np.mean((y_test_dl > 0) == (ensemble_pred > 0)) * 100
    print(f"  {name}: {dir_acc:.1f}%")
    if dir_acc > best_ensemble_acc:
        best_ensemble_acc = dir_acc
        best_ensemble_pred = ensemble_pred
        best_weights = name

print(f"\nüèÜ Best Ensemble: {best_weights} with {best_ensemble_acc:.1f}%")

# ============================================================
# üìä FINAL RESULTS
# ============================================================

def evaluate(y_true, y_pred, name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred)) * 100
    r2 = r2_score(y_true, y_pred)
    dir_acc = np.mean((y_true > 0) == (y_pred > 0)) * 100
    return {'Model': name, 'Dir Acc (%)': dir_acc, 'R¬≤': r2, 'RMSE (%)': rmse}

results = []
results.append(evaluate(y_test_dl, et_pred_aligned, 'Extra Trees'))
results.append(evaluate(y_test_dl, best_gru_pred, f'GRU ({best_gru_name})'))
results.append(evaluate(y_test_dl, best_ensemble_pred, f'Hybrid Ensemble'))

results_df = pd.DataFrame(results).sort_values('Dir Acc (%)', ascending=False)

print("\n" + "=" * 70)
print("üìä FINAL RESULTS")
print("=" * 70)
print(results_df.to_string(index=False))

# ============================================================
# üìà VISUALIZATION
# ============================================================

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy comparison
ax1 = axes[0]
colors = ['#2ecc71', '#3498db', '#9b59b6']
bars = ax1.bar(results_df['Model'], results_df['Dir Acc (%)'], color=colors)
ax1.axhline(y=50, color='red', linestyle='--', label='Random')
ax1.axhline(y=63.2, color='orange', linestyle='--', label='Previous Best (63.2%)')
ax1.set_title('Directional Accuracy Comparison', fontweight='bold')
ax1.set_ylabel('Accuracy (%)')
ax1.legend()
for bar, val in zip(bars, results_df['Dir Acc (%)']):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
             f'{val:.1f}%', ha='center', fontsize=11, fontweight='bold')

# Prediction vs Actual
ax2 = axes[1]
ax2.plot(y_test_dl[:100], 'b-', label='Actual', alpha=0.7)
ax2.plot(best_ensemble_pred[:100], 'r--', label='Predicted', alpha=0.7)
ax2.set_title('Actual vs Predicted Returns (First 100)', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/content/optimized_results.png', dpi=300)
plt.show()

# ============================================================
# üíæ SAVE
# ============================================================

import pickle
import os

save_path = '/content/drive/MyDrive/CSE_Final_Model'
os.makedirs(save_path, exist_ok=True)

best_gru_model.save(f'{save_path}/{COMPANY}_best_gru.keras')
with open(f'{save_path}/{COMPANY}_extra_trees.pkl', 'wb') as f:
    pickle.dump(et_model, f)
with open(f'{save_path}/{COMPANY}_scalers.pkl', 'wb') as f:
    pickle.dump({'scaler_X': scaler_X, 'scaler_y': scaler_y, 'features': feature_columns}, f)
results_df.to_csv(f'{save_path}/{COMPANY}_final_results.csv', index=False)

print(f"\n‚úÖ Models saved to {save_path}")

# ============================================================
# üìã FINAL SUMMARY
# ============================================================

print("\n" + "=" * 70)
print("üìã FINAL SUMMARY - OPTIMIZED MODEL")
print("=" * 70)

best = results_df.iloc[0]
print(f"\nüèÜ BEST RESULT: {best['Model']}")
print(f"   ‚Ä¢ Directional Accuracy: {best['Dir Acc (%)']:.1f}%")
print(f"   ‚Ä¢ R¬≤ Score: {best['R¬≤']:.4f}")

if best['Dir Acc (%)'] > 63.2:
    improvement = best['Dir Acc (%)'] - 63.2
    print(f"\n‚úÖ IMPROVED by {improvement:.1f}% over previous best (63.2%)!")
elif best['Dir Acc (%)'] >= 63:
    print(f"\n‚úÖ Matched previous best (~63%)!")
else:
    print(f"\n‚ö†Ô∏è Couldn't beat 63.2%. Try adding external data.")

print(f"\nüìä Total data used: {len(df)} records (10 years)")
print(f"üìä Features used: {len(feature_columns)}")
print("=" * 70)
